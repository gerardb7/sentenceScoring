{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def construct_context_gloss_pairs(input, target_start_id, target_end_id, lemma):\n",
    "    \"\"\"\n",
    "    construct context gloss pairs like sent_cls_ws\n",
    "    :param input: str, a sentence\n",
    "    :param target_start_id: int\n",
    "    :param target_end_id: int\n",
    "    :param lemma: lemma of the target word\n",
    "    :return: candidate lists\n",
    "    \"\"\"\n",
    "    sent = input.split(\" \")\n",
    "    assert 0 <= target_start_id and target_start_id < target_end_id  and target_end_id <= len(sent)\n",
    "    target = \" \".join(sent[target_start_id:target_end_id])\n",
    "    if len(sent) > target_end_id:\n",
    "        sent = sent[:target_start_id] + ['\"'] + sent[target_start_id:target_end_id] + ['\"'] + sent[target_end_id:]\n",
    "    else:\n",
    "        sent = sent[:target_start_id] + ['\"'] + sent[target_start_id:target_end_id] + ['\"']\n",
    "\n",
    "    sent = \" \".join(sent)\n",
    "    lemma = lemma\n",
    "\n",
    "\n",
    "    sense_data = pd.read_csv(\"/home/gerard/ownCloud/varis_tesi/GlossBERT_datasets/wordnet/index.sense.gloss\",sep=\"\\t\",header=None, error_bad_lines=False).values\n",
    "    d = dict()\n",
    "    for i in range(len(sense_data)):\n",
    "        s = sense_data[i][0]\n",
    "        pos = s.find(\"%\")\n",
    "        try:\n",
    "            d[s[:pos + 2]].append((sense_data[i][0],sense_data[i][-1]))\n",
    "        except:\n",
    "            d[s[:pos + 2]]=[(sense_data[i][0], sense_data[i][-1])]\n",
    "\n",
    "    # print(len(d))\n",
    "    # print(len(d[\"happy%3\"]))\n",
    "    # print(d[\"happy%3\"])\n",
    "\n",
    "    candidate = []\n",
    "    for category in [\"%1\", \"%2\", \"%3\", \"%4\", \"%5\"]:\n",
    "        query = lemma + category\n",
    "        try:\n",
    "            sents = d[query]\n",
    "            for sense_key, gloss in sents:\n",
    "                candidate.append((sent, f\"{target} : {gloss}\", target, lemma, sense_key, gloss))\n",
    "        except:\n",
    "            pass\n",
    "    assert len(candidate) != 0, f'there is no candidate sense of \"{lemma}\" in WordNet, please check'\n",
    "    print(f'there are {len(candidate)} candidate senses of \"{lemma}\"')\n",
    "\n",
    "\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-12_H-768_A-12 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-12_H-768_A-12 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "label_list = [\"0\", \"1\"]\n",
    "num_labels = len(label_list)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google/bert_uncased_L-12_H-768_A-12\", do_lower_case=True)\n",
    "model = BertForSequenceClassification.from_pretrained(\"google/bert_uncased_L-12_H-768_A-12\", num_labels=num_labels)\n",
    "model.to(device)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 7 candidate senses of \"plan\"\n"
     ]
    }
   ],
   "source": [
    "input = \"U.N. group drafts plan to reduce emissions\"\n",
    "target_start_id = 3\n",
    "target_end_id = 4\n",
    "lemma = \"plan\"\n",
    "\n",
    "candidate = construct_context_gloss_pairs(input, target_start_id, target_end_id, lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "candidate_results = []\n",
    "features = []\n",
    "for item in candidate:\n",
    "    text_a = item[0] # sentence\n",
    "    text_b = item[1] # gloss\n",
    "    candidate_results.append((item[-2], item[-1])) # (sense_key, gloss)\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(text_a)\n",
    "    tokens_b = tokenizer.tokenize(text_b)\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "    segment_ids = [0] * len(tokens)\n",
    "    tokens += tokens_b + [\"[SEP]\"]\n",
    "    segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    segment_ids += padding\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    features.append(\n",
    "        InputFeatures(input_ids=input_ids,\n",
    "                      input_mask=input_mask,\n",
    "                      segment_ids=segment_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0].input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbert",
   "language": "python",
   "name": "sbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
